# 텍스트 마이닝

## 머신러닝

> - 비정형데이터 -> 단어추출(형태소분석) -> 정형화(matrix, 되도록 수치형으로) -> 패턴/규칙 -> 학습모형
> - 텍스트가 주어지면 vectorization이 필요함
>   - BOW(Back Of Words) - 단어를 column으로 만들고 최소한의 단위(주로 문장)를 row로 만들어서 수치화 시킴 (몇개 포함되어 있는지)
>     - tf값 - 문서에 단순히 많이나온 단어들을 파악
>     - idf값 - 전체문서수 / 특정단어가 들어있는 문서의 개수 = 요약하자면 전체문서 중에 특정 문서에만 등장하는 정도(높을수록)
>     - 즉 tf x idf 가 높을 수록 문서를 구분하는데 도움되는 키워드임
>   - Word2vec - 단어간의 상관관계를 파악하고 싶을 때 (주로 챗봇에서 답변을 생성할 때 어떤 단어와 관련된 단어를 선정하는 로직)
>     - 학습을 시키는 데이터의 성향에 따라서 상관관계가 달라짐 (ex `어머니`에 대한 묘사가 다른 두 작품을 학습시키면 다른 성격을 묘사함)
> - 단어추출 (희소행렬)
>   - 형태소분석 -> 주요단어추출(명사만추출) -> 나온 모든 단어들로 리스트를 만듬 -> 이 리스트를 열로한 matrix를 만들어서 숫자를 셈
>     - 이경우에 열이 너무많고, 카운트할 때 0의 갯수가 너무 많기 때문에 비효율적임
>     - 그래서 COO모델, CSR모델 두 가지로 효율적으로 만들어 사용함
>       - COO방식
>         - 원본데이터 [[3,0,1], [0,2,0]]
>         - 합친데이터 (3,1,2)
>         - 행 좌표 (0, 0, 1)
>         - 열 좌표 (0, 2, 1)
>         - 출력 데이터  [[3,0,1], [0,2,0]]
> - 로지스틱 회귀분석을 주로 사용함 : 분석하고자하는 데이터의 성격이 잘 맞기때문에 정확도가 높게 나타남

## word_crawling.ipynb 문서 참고

> - CountVectorizer
>   - stop_words : 불용어로 지정 (단어를 카운팅할 때 제외하는 단어들)
>   - ngram_range : 토큰의 크기를 결정함 (기본은 1, 1, 추가로 범위지정도 가능함 (1, 2)는 1단어와 2단어를 토큰으로 인식)
>     - 모노그램 (1) : 한 단어를 한개의 토큰으로 인식함 (this is an apple : this, is, an, apple)
>     - 바이그램 (2) : 두 단어를 한개의 토큰으로 인식함 (this is an apple : this is, is an, an apple)
>   - token : 글자로 인식하고 싶은 최소단위 설정 (ex. word, character 등등)
>     - Token_pattern = "t/w+" 처럼 정규표현식으로 단어들을 선별할 수 있음
>   - 빈도수(max_df, min_df) : 한번에 들어가는 데이터 단위에서 토큰이 나타난 횟수를 지정하는 것
> - 매트릭스를 구성하는 방법을 잘 설정하는 것이 정확도에 큰 영향을 미친다.
> - GridSearchCV 
>   - 교차검증 (cross-validation)
>     - Cv = 3으로 설정하면 데이터 자체를 3등분 한 뒤 2개를 train, 1개를 test로 설정하여 총 3번 fitting하는 것임
>     - 그러다보니 속도가 느려져서 데이터가 많을 때는 잘 사용하지 않음

## word2vec

> - 워드 인베딩 기능 중의 하나 - 단어를 벡터화 시켜줌 (크기와 방향성을 갖는 정보로 바꿔줌)
> - Text -> Vectorization (전처리 과정)
>   1. BOW
>   2. One-hot-Encoding
>      - 단순히 원핫인코딩으로 거리를 나타내면 다 같은 거리값을 같게 되서 단어간의 관계를 파악하기 어려움
>   3. word embeding (word2vec)
>      - 단어들을 공간에 좌표화 시켜 단어간의 관계를 파악하는 기법
> - 

## 인공신경망 이론

> - 요약하자면 뉴런에 들어가는 X값들과 Y값은 알고 있기 때문에 각각의 자극들이 얼마나 들어갔는지 자극값(w)의 크기들을 구하는 것임
> - 결국은 다중회귀식으로 표현가능하며 판별함수(sigmoid)가 더해진 모습
> - w값을 찾을 때는 경사하강법을 사용함
> - 경사하강법을 사용할때는 전역해를 찾아야되기 때문에 x값을 표준화해주는 것이 중요함
> - 만약 수치예측을 하고 싶다면 판멸함수를 그냥 통과시켜주는 친구로 설정해주면 수치를 보여줌
> - 단층퍼셉트론 : 입력층 + 출력층
>   - 선형데이터는 분석가능하지만 비선형데이터는 분석하지 못함 (XOR문제)
> - 다층퍼셉트론(Multi-layer perceptron) : 입력층 + 중간층 + 출력층
>   - 중간층(Hidden layer)에서 똑같은 데이터를 다양한 관점으로 파악하고 그 관점들을 합산하여 비선형데이터를 분석하도록 함
>   - 데이터의 분포를 바꾸지는 않지만 관점을 다양하게 보면서 비선형데이터를 분석함
> - 손실함수(loss function or cost function)
>   1. 평균 제곱 오차 : 
>   2. 교차 엔트로피 오차 : 범주형데이터일 때도 사용가능
> - 역전파 알고리즘

## CBOW (continuous bag of words)

> - Center word를 중심으로 context words를 파악하고 원핫인코딩을 활용하여 표현함

![출처 : 데이터사이언스스쿨](https://datascienceschool.net/upfiles/1b6796e90e824dfe82910f0bb3ce47d5.png)

[출처 : 데이터사이언스스쿨](https://datascienceschool.net/upfiles/1b6796e90e824dfe82910f0bb3ce47d5.png)

> - skipgram방

