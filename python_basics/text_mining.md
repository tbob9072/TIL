# 텍스트 마이닝

## 머신러닝

> - 비정형데이터 -> 단어추출(형태소분석) -> 정형화(matrix, 되도록 수치형으로) -> 패턴/규칙 -> 학습모형
> - 텍스트가 주어지면 vectorization이 필요함
>   - BOW(Back Of Words) - 단어를 column으로 만들고 최소한의 단위(주로 문장)를 row로 만들어서 수치화 시킴 (몇개 포함되어 있는지)
>     - tf값 - 문서에 단순히 많이나온 단어들을 파악
>     - idf값 - 전체문서수 / 특정단어가 들어있는 문서의 개수 = 요약하자면 전체문서 중에 특정 문서에만 등장하는 정도(높을수록)
>     - 즉 tf x idf 가 높을 수록 문서를 구분하는데 도움되는 키워드임
>   - Word2vec - 단어간의 상관관계를 파악하고 싶을 때 (주로 챗봇에서 답변을 생성할 때 어떤 단어와 관련된 단어를 선정하는 로직)
>     - 학습을 시키는 데이터의 성향에 따라서 상관관계가 달라짐 (ex `어머니`에 대한 묘사가 다른 두 작품을 학습시키면 다른 성격을 묘사함)
> - 단어추출 (희소행렬)
>   - 형태소분석 -> 주요단어추출(명사만추출) -> 나온 모든 단어들로 리스트를 만듬 -> 이 리스트를 열로한 matrix를 만들어서 숫자를 셈
>     - 이경우에 열이 너무많고, 카운트할 때 0의 갯수가 너무 많기 때문에 비효율적임
>     - 그래서 COO모델, CSR모델 두 가지로 효율적으로 만들어 사용함
>       - COO방식
>         - 원본데이터 [[3,0,1], [0,2,0]]
>         - 합친데이터 (3,1,2)
>         - 행 좌표 (0, 0, 1)
>         - 열 좌표 (0, 2, 1)
>         - 출력 데이터  [[3,0,1], [0,2,0]]
> - 로지스틱 회귀분석을 주로 사용함 : 분석하고자하는 데이터의 성격이 잘 맞기때문에 정확도가 높게 나타남

## word_crawling.ipynb 문서 참고

> - CountVectorizer
>   - stop_words : 불용어로 지정 (단어를 카운팅할 때 제외하는 단어들)
>   - ngram_range : 토큰의 크기를 결정함 (기본은 1, 1, 추가로 범위지정도 가능함 (1, 2)는 1단어와 2단어를 토큰으로 인식)
>     - 모노그램 (1) : 한 단어를 한개의 토큰으로 인식함 (this is an apple : this, is, an, apple)
>     - 바이그램 (2) : 두 단어를 한개의 토큰으로 인식함 (this is an apple : this is, is an, an apple)
>   - token : 글자로 인식하고 싶은 최소단위 설정 (ex. word, character 등등)
>     - Token_pattern = "t/w+" 처럼 정규표현식으로 단어들을 선별할 수 있음
>   - 빈도수(max_df, min_df) : 한번에 들어가는 데이터 단위에서 토큰이 나타난 횟수를 지정하는 것
> - 매트릭스를 구성하는 방법을 잘 설정하는 것이 정확도에 큰 영향을 미친다.
> - GridSearchCV 
>   - 교차검증 (cross-validation)
>     - Cv = 3으로 설정하면 데이터 자체를 3등분 한 뒤 2개를 train, 1개를 test로 설정하여 총 3번 fitting하는 것임
>     - 그러다보니 속도가 느려져서 데이터가 많을 때는 잘 사용하지 않음

## word2vec

> - 워드 인베딩 기능 중의 하나 - 단어를 벡터화 시켜줌 (크기와 방향성을 갖는 정보로 바꿔줌)

