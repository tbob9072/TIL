# 주성분 분석 (PCA)

> - 주요 특성을 뽑을 때는 분산 값이 큰 특성들을 찾아서 판별에 활요하는 것이 효과적
> - 주요특성은 vector별로 파악함
> - 기존의 통계학과 다르게 정규분포를 따르는 자료들을 활용하는 것이 아니라 분산 커서 퍼져있는 자료들을 바탕으로 개별자료들을 분류하는 기법(굳이 따지자면 비지도 학습에 속함)
> - 특징들을 선형 결합시킬 때 데이터들이 가장 다양한 값들을 가지도록하는 특징의 가중치를 찾아주는 것
> - 벡터를 하나만 구하는 것이 아니라 좌표평면에서 벡터의 90도를 이루는 벡터를 구해주는 식으로 여러번 구한다.
> - **vector로 바꾸기 때문에 차원축소 효과를 볼 수 있음**
> - 학습을 하기전에 PCA를 사용하면 차원을 줄이기 때문에 Feature 수를 줄이는 효과가 있음 그렇기 때문에 over fitting을 막을 수 있음

```R
#PCA예제

x1<-c(26,46,57,36,57,26,58,37,36,56,78,95,88,90,52,56)
x2<-c(35,74,73,73,62,22,67,34,22,42,65,88,90,85,46,66)
x3<-c(35,76,38,69,25,25,87,79,36,26,22,36,58,36,25,44)
x4<-c(45,89,54,55,33,45,67,89,47,36,40,56,68,45,37,56)

score<-cbind(x1,x2,x3,x4)
colnames(score)<-c("국어","영어","수학","과학")
rownames(score)<-1:16
head(score)

#주성분분석(PCA)
result<-prcomp(score)
result
summary(result)
biplot(result)
screeplot(result,npcs=4,type="lines",main="Score")

#--------------------------결과------------------------------

> result
Standard deviations (1, .., p=4):
[1] 30.122748 27.052808  9.076140  6.152386

Rotation (n x k) = (4 x 4):
           PC1         PC2        PC3        PC4
국어 0.6093268 -0.39286407 -0.6126773 -0.3146508
영어 0.7185749 -0.09337973  0.6200124  0.3008572
수학 0.2624323  0.73573272  0.1052861 -0.6154198
과학 0.2085672  0.54372366 -0.4786711  0.6570680
#여기서 구해진 값은 w값들임 (PCA1 = 0.6093268*국어 + 0.7185749*영어 .....)
#값이 높을수록 잘 분별이 되는 것임 PC1의 경우에 국어와 영어는 식별이 잘됨 하지만 수학과 과학은 설명이 안됨
#pc2의 경우 반대로 수학, 과학이 설명이 잘됨

> summary(result)
Importance of components:
                           PC1     PC2     PC3     PC4
Standard deviation     30.1227 27.0528 9.07614 6.15239
Proportion of Variance  0.5157  0.4159 0.04682 0.02151
Cumulative Proportion   0.5157  0.9317 0.97849 1.00000

#결과를 바탕으로 어느 차원까지 사용해야 할 지 정해야 하는데 summary에서 PC1과 PC2의 Standard deviation을 보고 설명력을 판단하고(높을 수록 좋음), Cumulative Proportion을 바탕으로 누적기여율을 바탕으로 0.8이상이 되는 차원까지 사용한다.

```

